#!/usr/bin/env python3
"""
Basic usage example for the Self-Healing MLOps Bot.

This example demonstrates how to:
1. Set up the bot for a repository
2. Configure detectors and playbooks
3. Handle webhook events
4. Monitor bot health
"""

import asyncio
import logging
from pathlib import Path

from self_healing_bot import SelfHealingBot
from self_healing_bot.core.context import Context
from self_healing_bot.core.playbook import Playbook, Action, PlaybookRegistry
from self_healing_bot.detectors.base import BaseDetector

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CustomFailureDetector(BaseDetector):
    """Example custom detector for specific failure patterns."""
    
    def get_supported_events(self):
        return ["workflow_run", "check_run"]
    
    async def detect(self, context: Context):
        """Detect custom failure patterns."""
        issues = []
        
        # Example: Check for specific error patterns
        if context.event_type == "workflow_run":
            workflow_data = context.event_data.get("workflow_run", {})
            if workflow_data.get("conclusion") == "failure":
                # Mock check for custom error pattern
                issues.append({
                    "type": "custom_failure",
                    "severity": "high",
                    "message": "Custom failure pattern detected",
                    "data": {
                        "workflow_name": workflow_data.get("name"),
                        "failure_reason": "custom_error_pattern"
                    }
                })
        
        return issues


@PlaybookRegistry.register("custom_repair")
class CustomRepairPlaybook(Playbook):
    """Example custom playbook for automated repairs."""
    
    def should_trigger(self, context: Context):
        """Trigger when custom failure is detected."""
        return context.has_error() and "custom" in context.error_message.lower()
    
    @Action(order=1)
    def analyze_failure(self, context: Context):
        """Analyze the failure and gather information."""
        logger.info("Analyzing custom failure...")
        
        # Mock analysis
        context.set_state("analysis_result", "dependency_version_mismatch")
        context.set_state("suggested_fix", "update_requirements.txt")
        
        return "Analysis completed: dependency version mismatch detected"
    
    @Action(order=2)  
    def apply_fix(self, context: Context):
        """Apply the suggested fix."""
        suggested_fix = context.get_state("suggested_fix")
        logger.info(f"Applying fix: {suggested_fix}")
        
        if suggested_fix == "update_requirements.txt":
            # Mock file update
            updated_requirements = """
# Updated requirements with compatible versions
numpy>=1.21.0,<1.22.0
pandas>=1.3.0,<1.4.0
scikit-learn>=1.0.0,<1.1.0
""".strip()
            
            context.write_file("requirements.txt", updated_requirements)
            return "Updated requirements.txt with compatible versions"
        
        return "No applicable fix found"
    
    @Action(order=3)
    def create_pr(self, context: Context):
        """Create a pull request with the fix."""
        analysis_result = context.get_state("analysis_result")
        
        pr = context.create_pull_request(
            title="ðŸ¤– Fix dependency version mismatch",
            body=f"""
## Automated Fix

The self-healing bot detected and fixed a dependency version mismatch issue.

### Analysis Result
{analysis_result}

### Changes Made
- Updated requirements.txt with compatible package versions
- Resolved version conflicts that were causing build failures

### Testing
Please verify that the updated dependencies work correctly with your code.

---
*This PR was automatically generated by the self-healing MLOps bot*
            """,
            branch="fix/auto-dependency-update"
        )
        
        return f"Created PR #{pr.number} with automated fix"


async def main():
    """Main example demonstrating bot usage."""
    logger.info("Starting Self-Healing MLOps Bot example")
    
    # Initialize the bot
    bot = SelfHealingBot()
    
    # Register custom detector
    custom_detector = CustomFailureDetector()
    bot.detector_registry.register_detector("custom_failure", custom_detector)
    
    # Example webhook event (workflow failure)
    webhook_event = {
        "action": "completed",
        "workflow_run": {
            "id": 123456,
            "name": "CI/CD Pipeline",
            "status": "completed",
            "conclusion": "failure",
            "html_url": "https://github.com/example/repo/actions/runs/123456"
        },
        "repository": {
            "full_name": "example/ml-project",
            "name": "ml-project",
            "owner": {"login": "example"}
        }
    }
    
    try:
        # Process the webhook event
        logger.info("Processing webhook event...")
        context = await bot.process_event("workflow_run", webhook_event)
        
        if context:
            logger.info(f"Event processed successfully: {context.execution_id}")
            
            # Check if any issues were detected
            if context.has_error():
                logger.warning(f"Issues detected: {context.error_message}")
            
            # Show file changes made by playbooks
            file_changes = context.get_file_changes()
            if file_changes:
                logger.info("Files modified by automated repairs:")
                for file_path, content in file_changes.items():
                    logger.info(f"  - {file_path}")
                    logger.debug(f"    Content preview: {content[:100]}...")
        
        # Health check
        logger.info("Checking bot health...")
        health_status = await bot.health_check()
        logger.info(f"Bot health: {health_status['status']}")
        
        # Show component status
        for component, status in health_status["components"].items():
            logger.info(f"  {component}: {status}")
            
    except Exception as e:
        logger.error(f"Error processing event: {e}")
        raise


def setup_repository_config():
    """Example of setting up repository configuration."""
    config_content = """
# Self-healing bot configuration for ML repository
version: 1.0

# Monitoring configuration
monitoring:
  pipelines:
    - name: "ml-training-pipeline"
      type: "github-actions"
      workflow: ".github/workflows/train.yml"
      success_rate_threshold: 0.95
      
    - name: "model-deployment"
      type: "github-actions" 
      workflow: ".github/workflows/deploy.yml"
      success_rate_threshold: 0.98
      
  models:
    - name: "production-classifier"
      endpoint: "https://api.example.com/v1/predict"
      metrics:
        - name: "accuracy"
          threshold: 0.92
          window: "24h"
        - name: "latency_p99"
          threshold: 500  # ms
          
  data:
    - name: "training-dataset"
      path: "data/processed/"
      drift_threshold: 0.1
      check_frequency: "daily"

# Playbook configuration
playbooks:
  - trigger: "test_failure"
    actions:
      - "analyze_logs"
      - "fix_common_errors"
      - "create_pr"
      
  - trigger: "custom_failure"
    actions:
      - "analyze_failure"
      - "apply_fix"
      - "create_pr"
      
  - trigger: "data_drift"
    actions:
      - "validate_data_quality"
      - "trigger_retraining"
      - "notify_team"

# Notification settings
notifications:
  slack:
    webhook: "${SLACK_WEBHOOK_URL}"
    channels:
      failures: "#ml-alerts"
      repairs: "#ml-ops"
"""
    
    # Write configuration file
    config_path = Path(".github/self-healing-bot.yml")
    config_path.parent.mkdir(parents=True, exist_ok=True)
    config_path.write_text(config_content)
    
    logger.info(f"Repository configuration written to {config_path}")


def create_custom_playbook_example():
    """Example of creating a custom playbook for specific ML issues."""
    
    @PlaybookRegistry.register("gpu_memory_optimizer")
    class GPUMemoryOptimizer(Playbook):
        """Optimize GPU memory usage when OOM errors occur."""
        
        def should_trigger(self, context: Context):
            return (
                context.has_error() and 
                "cuda out of memory" in context.error_message.lower()
            )
        
        @Action(order=1)
        def reduce_batch_size(self, context: Context):
            """Reduce batch size to fit in available GPU memory."""
            # Load current training config
            config = context.load_config("config/training.yaml")
            
            current_batch_size = config.get("batch_size", 32)
            new_batch_size = max(1, current_batch_size // 2)
            
            config["batch_size"] = new_batch_size
            context.save_config("config/training.yaml", config)
            context.set_state("old_batch_size", current_batch_size)
            context.set_state("new_batch_size", new_batch_size)
            
            return f"Reduced batch size from {current_batch_size} to {new_batch_size}"
        
        @Action(order=2)
        def enable_mixed_precision(self, context: Context):
            """Enable mixed precision training to reduce memory usage."""
            training_script = context.read_file("train.py")
            
            if "autocast" not in training_script:
                # Add mixed precision training
                mixed_precision_code = """
# Enable mixed precision training
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# In training loop:
with autocast():
    outputs = model(inputs)
    loss = criterion(outputs, targets)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
"""
                updated_script = training_script + "\n" + mixed_precision_code
                context.write_file("train.py", updated_script)
                return "Enabled mixed precision training"
            
            return "Mixed precision already enabled"
        
        @Action(order=3)
        def create_optimization_pr(self, context: Context):
            """Create PR with GPU memory optimizations."""
            old_batch_size = context.get_state("old_batch_size")
            new_batch_size = context.get_state("new_batch_size")
            
            pr = context.create_pull_request(
                title="ðŸ¤– Optimize GPU memory usage",
                body=f"""
## GPU Memory Optimization

The self-healing bot detected GPU out-of-memory errors and applied optimizations:

### Changes Made
- Reduced batch size from {old_batch_size} to {new_batch_size}
- Enabled mixed precision training
- Estimated memory savings: ~40%

### Impact
- Training will take longer but use less GPU memory
- Model accuracy should remain unchanged
- Consider upgrading to larger GPU instances if needed

---
*This PR was automatically generated by the self-healing MLOps bot*
                """,
                branch="fix/gpu-memory-optimization"
            )
            
            return f"Created optimization PR #{pr.number}"
    
    logger.info("Custom GPU memory optimizer playbook registered")


if __name__ == "__main__":
    # Set up repository configuration
    setup_repository_config()
    
    # Create custom playbook example
    create_custom_playbook_example()
    
    # Run the main example
    asyncio.run(main())